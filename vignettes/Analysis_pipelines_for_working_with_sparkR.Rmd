---
title: "Analysis pipelines for working with Spark DataFrames for one-time/ batch analyses"
author: "Naren S, Anoop S"
date: "11/13/2018"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 8
vignette: >
  %\VignetteIndexEntry{Analysis pipelines for working with Spark DataFrames for batch analyses}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Introduction

*Apache Spark* can be leveraged to process large volumes of distributed data that are typically impossible to process on standalone R servers. The vignette describes defining and executing *Spark*-only pipelines using the *analysisPipelines* package.

# Initialize libraries

* Load the *analysisPipelines* and *SparkR* libraries
* Ensure you have a local installation of spark and SparkR package is installed
* Check is SPARK_HOME environment variable is set to spark installation folder. Else, define it using `sys.setenv()` function.
```{r, include=FALSE}
knitr::opts_chunk$set(
    eval = FALSE
  )
library(ggplot2)
library(analysisPipelines)
library(SparkR)

## Define these variables as per the configuration of your machine
sparkHome <- "/Users/naren/softwares/spark-2.3.1-bin-hadoop2.7/"
sparkMaster <- "local[1]"
sparkPackages <- c("org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1")
# Set spark home variable if not present
if(Sys.getenv("SPARK_HOME") == "") {
  Sys.setenv(SPARK_HOME = sparkHome)  
}
```

# Connect to Spark cluster

* Define the spark master URL
* Specify dependency packages if any during spark connection. Example: `sparkPackages <- c("org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1")`
* Connect to the cluster using the package's `sparkRSessionCreateIfNotPresent` function

```{r}
sparkRSessionCreateIfNotPresent(master = sparkMaster, sparkPackages = sparkPackages)
```

# Read data from csv and initialize pipeline object
Spark can connect to datasources like Hive, Kafka. Besides, it also also read parquet, json and csv files. In this example we will read a csv file.
```{r}
pipelineObj <- AnalysisPipeline(filePath = system.file("hotel_new.csv", package = "analysisPipelines"))
# pipelineObj <- AnalysisPipeline(input = SparkR::as.data.frame(system.file("hotel_new.csv", package = "analysisPipelines")))
```

# User-defined spark functions
The example below shows a few functions to perform simple aggregations.

```{r}
# Function to calculate avg occupancy for the client as well as competitors - grouped by input parameter
occupancyAnalysis <- function(inputDataset, groupByColumn) {
  occupancySummary <- SparkR::summarize( SparkR::groupBy(inputDataset,inputDataset[[groupByColumn]]),
                       avgOccupancy =  SparkR::mean(inputDataset$Occupancy),
                       avgCompetitorOccupancy=  SparkR::mean(inputDataset$Compet_Occupancy))
 return(occupancySummary)
}

# Function to calculate avg booking contribution for differen channels - grouped by input parameter
bookingChannelsAnalysis <- function(inputDataset, groupByColumn) {
  bookingChannelsAnalysisSummary <-  SparkR::summarize( SparkR::groupBy(inputDataset,inputDataset[[groupByColumn]]),
                       avgCallCenterBookingPct =  SparkR::avg(inputDataset$call_center_booking_pct),
                       avgAgencyBookingPct = SparkR::avg(inputDataset$travel_agency_booking_pct),
                       avgThirdPartyBookingPct =  SparkR::avg(inputDataset$third_party_web_bookings_pct),
                       avgHotelToHotelBookingPct = SparkR::avg(inputDataset$hotel_to_hotel_bookings_pct),
                       avgDirectBookingPct = SparkR::avg(inputDataset$direct_call_booking_pct),
                       avgWebBookingPct = SparkR::avg(inputDataset$direct_web_booking_pct))
 return(bookingChannelsAnalysisSummary)
}

```

# Registering user-defined functions to the pipeline object

Each user-defined function needs to be registered to the pipeline object. Post registration, the function can be used to construct a pipeline. A pipeline is a set of multiple functions called in a particular sequence.

```{r}
# Register user-defined functions
registerFunction("occupancyAnalysis", "Occupany analysis",
                                                 engine = "spark")

registerFunction("bookingChannelsAnalysis", "Booking channels analysis",
                                                 engine = "spark")
# List all registered functions 
getRegistry()

# Define pipeline from list of registered functions
pipelineObj %>% occupancyAnalysis(groupByColumn = "location_type", storeOutput = T) %>% 
                bookingChannelsAnalysis(groupByColumn = "location_type", storeOutput = T) -> pipelineObj

pipelineObj %>>% getPipeline
pipelineObj %>>% visualizePipeline
```

# Running the pipeline and generating an output

The pipeline is run by calling the `generateOutput()` function. A particular output in the sequence on evaluations can be accessed by calling the `getOutputById` function


```{r fig.width=6, fig.height=3}
pipelineObj %>% generateOutput -> pipelineObj

# Show occupancy analysis result
occupancyAnalysisResult <- pipelineObj %>>% getOutputById(1)
occupancyAnalysisResultDf <- as.data.frame(occupancyAnalysisResult)
DT::datatable(head(occupancyAnalysisResultDf),options = list(scrollX = T, scrollY = T))


# ggplot2::ggplot(occupancyAnalysisResultDf) + ggplot2::geom_col(ggplot2::aes(location_type, avgOccupancy, color=location_type)) 

# Show booking channels analysis result
bookingChannelsAnalysisResult <- pipelineObj %>>% getOutputById(2)
bookingChannelsAnalysisResultDf <- as.data.frame(bookingChannelsAnalysisResult)
DT::datatable(head(bookingChannelsAnalysisResultDf),options = list(scrollX = T, scrollY = T))


# ggplot2::ggplot(bookingChannelsAnalysisResultDf) + ggplot2::geom_col(ggplot2::aes(location_type, avgCallCenterBookingPct, color=location_type)) + ggplot2::scale_y_continuous(labels = scales::percent)

```

# Supplementary Note

The *analysisPipelines* package internally uses the *SparkR* package to interface with *Spark*. *SparkR* masks many typical data manipulation and processing functions from *base* as well as packages like *dplyr*. Therefore, ensure you use function scoping when calling a function.
