---
title: "Analysis pipelines for working with Spark DataFrames for one-time/ batch analyses"
author: "Naren S, Anoop S"
date: "09/04/2018"
# output: rmarkdown::html_vignette
# vignette: >
#   %\VignetteIndexEntry{Analysis pipelines for working with SparkR}
#   %\VignetteEngine{knitr::rmarkdown}
#   %\VignetteEncoding{UTF-8}
---
# Objective

The vignette aims to show examples of using SparkR as an interface to run one-time/batch spark jobs through R - using the analysisPipelines package. The major use cases can be categorized as:<br>

* Implement a pipeline using SparkR, to work on Spark DataFrames for one-time/batchtch Spark jobs

## Pipelines for batch spark jobs using SparkR

Spark clusters can be leveraged to process large volumes of distributed data that are typically impossible to process on standlone R servers. This example illustrates usage of pipeline functions to run one-time/batch Spark jobs using the analysisPipelines package

### Initialize libraries

* Initialize the analysisPipelines and SparkR libraries
* Ensure you have a local installation of spark and SparkR package is installed
* Check is SPARK_HOME environment variable is set to spark installation folder. Else, define it using `sys.setenv()` function.
```{r, include=FALSE}
library(ggplot2)
library(analysisPipelines)
library(SparkR)

sparkHome <- "/Users/naren/softwares/spark-2.3.1-bin-hadoop2.7/"
sparkMaster <- "local[1]"
sparkPackages <- c("org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1")
# Set spark home variable if not present
if(Sys.getenv("SPARK_HOME") == "") {
  Sys.setenv(SPARK_HOME = sparkHome)  
}
```

### Connect to spark cluster
* Define the spark master URL
* Specify dependency packages if any during spark connection. Example: `sparkPackages <- c("org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1")`
* Connect to the cluster using the package's `sparkRSessionCreateIfNotPresent` function

```{r}
sparkRSessionCreateIfNotPresent(master = sparkMaster, sparkPackages = sparkPackages)
```

### Read data from csv and initialize spark pipeline object
Spark can connect to datasources like Hive, Kafka. Besides, it also also read parquet, json and csv files. In this example we will read a csv file.
```{r}
inputDataset <- read.df(path="../inst/hotel_new.csv",source="csv",header = TRUE, inferSchema = "true")
pipelineObj <- AnalysisPipeline(input = SparkR::as.data.frame(inputDataset))
```

### User defined spark functions
The example below shows a few functions to perform simple aggregations.

```{r}
# Function to calculate avg occupany for the client as well as competitors - grouped by input parameter
occupanyAnalysis <- function(inputDataset, groupByColumn) {
  occupancySummary <- summarize(groupBy(inputDataset,inputDataset[[groupByColumn]]),
                       avgOccupancy=mean(inputDataset$Occupancy),
                       avgCompetitorOccupancy=mean(inputDataset$Compet_Occupancy))
 return(occupancySummary)
}

# Function to calculate avg booking contribution for differen channels - grouped by input parameter
bookingChannelsAnalysis <- function(inputDataset, groupByColumn) {
  bookingChannelsAnalysisSummary <- summarize(groupBy(inputDataset,inputDataset[[groupByColumn]]),
                       avgCallCenterBookingPct=avg(inputDataset$call_center_booking_pct),
                       avgAgencyBookingPct=avg(inputDataset$travel_agency_booking_pct),
                       avgThirdPartyBookingPct=avg(inputDataset$third_party_web_bookings_pct),
                       avgHotelToHotelBookingPct=avg(inputDataset$hotel_to_hotel_bookings_pct),
                       avgDirectBookingPct=avg(inputDataset$direct_call_booking_pct),
                       avgWebBookingPct=avg(inputDataset$direct_web_booking_pct))
 return(bookingChannelsAnalysisSummary)
}

```

### Registering user defined functions to the pipeline object

Each user defined function needs to be registered to the pipeline object. Post registration, the function can be used to construct a pipeline. A pipeline is a set of multiple functions called in a particular sequence.

```{r}
# Register user defined functions
pipelineObj <- pipelineObj %>>% registerFunction("occupanyAnalysis", "Occupany analysis",outAsIn = F,
                                                 engine = "spark")  %>>%
  registerFunction("bookingChannelsAnalysis", "Booking channels analysis", outAsIn = F,
                                                 engine = "spark")
# List al registered functions 
pipelineObj %>>% getRegistry

# Define pipeline from list of registered functions
pipelineObj %>% occupanyAnalysis(groupByColumn = "location_type") %>% bookingChannelsAnalysis(groupByColumn = "location_type") -> pipelineObj

pipelineObj %>>% getPipeline
```

### Running the pipeline and generating an output

The pipeline is run by calling the `generateOutput()` function. A particular output in the sequence on evaluations can be accessed by calling the `getOutputByOrderId` function

In this example the Spark DataFrames are converted to R dataframes to help visualize the result.

```{r fig.width=6, fig.height=3}
pipelineObj %>% generateOutput() -> pipelineObj

# Show occupancy analysis result
occpancyAnalysisResult <- pipelineObj %>>% getOuputByOrderId(1)
occpancyAnalysisResultDf <- as.data.frame(occpancyAnalysisResult)
occpancyAnalysisResultDf

# ggplot(occpancyAnalysisResultDf) + geom_col(aes(location_type, avgOccupancy, color=location_type)) 

# Show booking channels analysis result
bookingChannelsAnalysisResult <- pipelineObj %>>% getOuputByOrderId(2)
bookingChannelsAnalysisResultDf <- as.data.frame(bookingChannelsAnalysisResult)
bookingChannelsAnalysisResultDf
# ggplot(bookingChannelsAnalysisResultDf) + geom_col(aes(location_type, avgCallCenterBookingPct, color=location_type)) + scale_y_continuous(labels = scales::percent)

```

