---
title: "Analysis pipelines for working with Spark DataFrames for one-time/ batch analyses"
author: "Naren S, Anoop S"
date: "11/13/2018"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 8
vignette: >
  %\VignetteIndexEntry{Analysis pipelines for working with Spark DataFrames for batch analyses}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Introduction

*Apache Spark* can be leveraged to process large volumes of distributed data that are typically impossible to process on standalone R servers. The vignette describes defining and executing *Spark*-only pipelines using the *analysisPipelines* package.

# Important Note

Using *Spark* as an engine requires the *SparkR* package to be installed. *SparkR* is distributed natively with *Apache Spark* and is not distributed on CRAN. The *SparkR* version needs to directly map to the Spark version (hence the native distribution), and care needs to be taken to ensure that this is configured properly.

To install from Github, run the following command, if you know the Spark version:

```{r eval = F}
devtools::install_github('apache/spark@v2.x.x', subdir='R/pkg')
```

The other option is to install R by running the following *terminal* commands if Spark has already been installed.

```{bash eval = F}
$ export SPARK_HOME=/path/to/spark/directory
$ cd $SPARK_HOME/R/lib/SparkR/
$ R -e "devtools::install('.')"
```


# Initialize libraries

* Load the *analysisPipelines* and *SparkR* libraries
* Check if the  SPARK_HOME environment variable is set to spark installation folder. Else, define it using `sys.setenv()` function.

```{r, include=FALSE}
knitr::opts_chunk$set(
    eval = TRUE
  )
library(ggplot2)
library(analysisPipelines)
library(SparkR)

## Define these variables as per the configuration of your machine
sparkHome <- "/Users/naren/softwares/spark-2.3.1-bin-hadoop2.7/"
sparkMaster <- "local[1]"
sparkPackages <- c("org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1")
# Set spark home variable if not present
if(Sys.getenv("SPARK_HOME") == "") {
  Sys.setenv(SPARK_HOME = sparkHome)  
}
```

# Connect to Spark cluster

* Define the spark master URL
* Specify dependency packages if any during spark connection. Example: `sparkPackages <- c("org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1")`
* Connect to the cluster using the package's `sparkRSessionCreateIfNotPresent` function

```{r}
sparkRSessionCreateIfNotPresent(master = sparkMaster, sparkPackages = sparkPackages)
```

# Read data from csv and initialize pipeline object

Spark can connect to datasources like Hive, Kafka. Besides, it can also read parquet, json and csv files. In this example we will read a csv file.

```{r}
pipelineObj <- AnalysisPipeline(filePath = system.file("hotel_new.csv", package = "analysisPipelines"))
```

# User-defined spark functions
The example below shows a few functions to perform simple aggregations.

```{r}
# Function to calculate avg occupancy for the client as well as competitors - grouped by input parameter
occupancyAnalysis <- function(inputDataset, groupByColumn) {
  occupancySummary <- SparkR::summarize( SparkR::groupBy(inputDataset,inputDataset[[groupByColumn]]),
                       avgOccupancy =  SparkR::mean(inputDataset$Occupancy),
                       avgCompetitorOccupancy=  SparkR::mean(inputDataset$Compet_Occupancy))
 return(occupancySummary)
}

# Function to calculate avg booking contribution for differen channels - grouped by input parameter
bookingChannelsAnalysis <- function(inputDataset, groupByColumn) {
  bookingChannelsAnalysisSummary <-  SparkR::summarize( SparkR::groupBy(inputDataset,inputDataset[[groupByColumn]]),
                       avgCallCenterBookingPct =  SparkR::avg(inputDataset$call_center_booking_pct),
                       avgAgencyBookingPct = SparkR::avg(inputDataset$travel_agency_booking_pct),
                       avgThirdPartyBookingPct =  SparkR::avg(inputDataset$third_party_web_bookings_pct),
                       avgHotelToHotelBookingPct = SparkR::avg(inputDataset$hotel_to_hotel_bookings_pct),
                       avgDirectBookingPct = SparkR::avg(inputDataset$direct_call_booking_pct),
                       avgWebBookingPct = SparkR::avg(inputDataset$direct_web_booking_pct))
 return(bookingChannelsAnalysisSummary)
}

```

# Registering user-defined functions to the pipeline object

Each user-defined function needs to be registered to the pipeline object. For non-R engines, such as Spark and Python, a suffix with the engine name is added to the function name on registration. So, functions with this suffix need to be used when pipelining to an *Analysis Pipeline* object. The engine is added as a suffix for better readability. A suffix is used (as opposed to a prefix) to enable easier auto-completes.

Post registration, the function can be used to construct a pipeline. A pipeline is a set of multiple functions called in a particular sequence.

```{r}
# Register user-defined functions
registerFunction("occupancyAnalysis", "Occupany analysis",
                                                 engine = "spark")

registerFunction("bookingChannelsAnalysis", "Booking channels analysis",
                                                 engine = "spark")
# List all registered functions 
getRegistry()

# Define pipeline from list of registered functions
pipelineObj %>% occupancyAnalysis_spark(groupByColumn = "location_type", storeOutput = T) %>% 
                bookingChannelsAnalysis_spark(groupByColumn = "location_type", storeOutput = T) -> pipelineObj

pipelineObj %>>% getPipeline
pipelineObj %>>% visualizePipeline
```

# Running the pipeline and generating an output

The pipeline is run by calling the `generateOutput()` function. A particular output in the sequence on evaluations can be accessed by calling the `getOutputById` function


```{r fig.width=6, fig.height=3}
pipelineObj %>% generateOutput -> pipelineObj

# Show occupancy analysis result
occupancyAnalysisResult <- pipelineObj %>>% getOutputById(1)
occupancyAnalysisResultDf <- as.data.frame(occupancyAnalysisResult)
DT::datatable(head(occupancyAnalysisResultDf),options = list(scrollX = T, scrollY = T))

# Show booking channels analysis result
bookingChannelsAnalysisResult <- pipelineObj %>>% getOutputById(2)
bookingChannelsAnalysisResultDf <- as.data.frame(bookingChannelsAnalysisResult)
DT::datatable(head(bookingChannelsAnalysisResultDf),options = list(scrollX = T, scrollY = T))
```

# Supplementary Note

The *analysisPipelines* package internally uses the *SparkR* package to interface with *Spark*. *SparkR* masks many typical data manipulation and processing functions from *base* as well as packages like *dplyr*. Therefore, ensure you use function scoping when calling a function.
