Package: analysisPipelines
Type: Package
Title: Compose interoperable analysis pipelines, and put them into production
Version: 0.0.0.9003
Authors@R: c(
      person("Naren","Srinivasan", email = "Naren.Srinivasan@mu-sigma.com", role = c("aut")),
      person("Zubin Dowlaty","",  email = "Zubin.Dowlaty@mu-sigma.com", role = c("ctb")),
      person("Sanjay","",  email = "Sanjay@mu-sigma.com", role = c("ctb")),
      person("Neeratyoy","Mallik", email = "Neeratyoy.Mallik@mu-sigma.com", role = c("ctb")),
		  person("Anoop S","",  email = "Anoop.S@mu-sigma.com", role = c("ctb")),
		  person("Mu Sigma, Inc.", email = "ird.experiencelab@mu-sigma.com", role = c("cre"))
		  )
Description: The package aims at enabling data scientists to compose pipelines of analysis which consist of data manipulation, exploratory analysis & reporting, as well as modeling steps. It also aims to enable data scientists to use tools of their choice through an R interface, and compose interoperable pipelines between R, Spark, and Python. Credits to Mu Sigma for supporting the development of the package.
Depends: R (>= 3.4.0), tibble, magrittr, data.table, pipeR, devtools
Imports: ggplot2, dplyr, futile.logger, RCurl, proto
Suggests: plotly, knitr, rmarkdown, SparkR, parallel, visNetwork, rjson, DT, shiny
Remotes: github::cran/SparkR
Encoding: UTF-8
License: Apache License 2.0
LazyLoad: yes
LazyData: yes
RoxygenNote: 6.1.1
VignetteBuilder: knitr
Collate: 
    'analysisPipelines_package.R'
    'core-functions.R'
    'core-functions-batch.R'
    'core-functions-meta-pipelines.R'
    'core-streaming-functions.R'
    'r-batch-eda-utilities.R'
    'spark-structured-streaming-utilities.R'
    'zzz.R'
